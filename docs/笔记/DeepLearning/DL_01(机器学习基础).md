> First version on 2021-12-16
>
>Last version on 2021-12-20
>
>Author :FeynmanR

- [机器学习基础](#机器学习基础)
  - [人工智能](#人工智能)
  - [开发一个人工智能系统](#开发一个人工智能系统)
  - [机器四要素（以回归为例）](#机器四要素以回归为例)
    - [**数据**](#数据)
    - [**模型**](#模型)
    - [**学习准则**](#学习准则)
    - [**优化算法**](#优化算法)
  - [泛化与正则化](#泛化与正则化)

# 机器学习基础

## 人工智能
> 人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样——John McCarthy

人工智能(AI)在达特茅斯会议上定义的就是希望机器能表现出和人类一样的智能，但具体是通过什么途径的，并没有给出明确要求。人类在不同方向上进行了探索，形成了各种流派。主要流派有：连接主义，符号主义，行为主义。

为了使得计算机能通过著名的图灵测试，计算机必须具备理解语言，学习，记忆，推理，决策等能力。由此人工智能所研究的领域主要有：

- 机器感知（计算机视觉，语音信息处理，模式识别）
- 学习（机器学习，强化学习）
- 语言（自然语言处理）
- 记忆（知识表示）
- 决策（规划，数据挖掘）

## 开发一个人工智能系统

对于早期的人工智能系统，人们采用了专家系统这种方式，处理某一类较为简单的问题，人们知道如何解决，也知道如何让机器实现，这时候把专家的知识教给机器就可以了。但对于某些问题，我们不知道怎么让机器实现，比如：语音信号，手写体字符。对于机器来说还是没有统一的方法，但对人类却很容易处理，这时候我们采用机器学习的方法，通过人为打标注来，运用机器学习的方法让机器学习知识。对于连人类也不容易做的事情，比如下围棋等，可以使用强化学习的方法。结构如下图所示

![](https://gitee.com/feynmanrp/img/raw/master/img/20211212154836.png)


## 机器四要素（以回归为例）
### **数据**

### **模型**

$$f(x;\theta)=\omega^Tx+b$$

其中$\theta$是模型所要求的参数，这里指$\omega$和b
### **学习准则**
损失函数：损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异

常用的有平方损失函数（之后会介绍更多）：

$$L(y,f(x;\theta))=\frac{1}{2}(y-f(x;\theta))^2$$

期望风险：是指在真实的数据分布下的损失函数的期望最小化

由于在现实中不可能得到真实的数据分布，那怎么办呢？根据大数定理，可以对数据采样计算风险，这里被采样的数据被称为训练数据，得到的风险叫做经验风险

$$R_D^{emp}(\theta)=\frac{1}{N}\sum_{n=1}^NL(y^{(n)},f(x^{(n)};\theta))$$

机器学习的目的是找到一个$\theta^*$
，使得经验风险函数最小化
### **优化算法**
凸优化与非凸优化：对于凸优化问题，只要找到一个点导数为0的点，这个点就是全局最小值。但对于非凸优化问题是比较困难的，存在全局最优和局部最优。

我们在目前的机器学习中如何使用比较高级的非凸优化算法是一个重要问题

**非凸优化方法举例**

梯度下降法（Gradient Descent）:简单来说往梯度的反方向调整参数大小

$$\theta_{t+1}=\theta_t-\alpha\frac{\partial R_D{\theta}}{\partial\theta}$$

$$=\theta_t-\alpha\frac{1}{N}\sum_{n=1}^N\frac{\partial L(y^{(n)},f(x^{(n)};\theta))}{\partial\theta}$$

其中$\alpha也叫做学习率$，也是一个重要的超参数

随机梯度下降法（Stochastic Gradient Descent,SGD）：在每次迭代时只采集一个样本

优点：计算开销小，支持在线学习

缺点：无法充分利用计算机的并行计算能力

小批量（Mini-Batch）随机梯度下降法:随机选取一小部分训练样本来计算梯度并更新参数

既可以兼顾随机梯度下降的优点，也可以提高训练效率

## 泛化与正则化
过拟合：经验风险最小化原则很容易导致模型在训练集上错误率很低，但是在未知数据上错误率很高。

过拟合问题往往是由于训练数据少和噪声等原因造成的(造成了期望风险与经验风险差异过大)。

泛化误差：

$$G_D(f)=R(f)-R_D^{emp}(f)$$

那么如何减少泛化误差呢？一个有效的手段是正则化

正则化：所有损害优化的方法都是正则化。可以增加优化约束（L1/L2约束，数据增强），也可以干扰过程（权重衰减，随机梯度下降，提前停止）

提前停止：我们使用一个验证集来测试每一次迭代的参数在验证集上是否最优。如果在验证集上的错误率不再下降，就停止迭代。