> First version on 2021-1-4
>
>Last version on 2022-1-6
>
>Author :FeynmanRP


# 网络优化与正则化

## 网络优化

### 网络优化的难点
- 结构差异大
  - 没有通用的优化算法
  - 超参数多
- 非凸优化问题
  - 参数初始化
  - 逃离局部最优或鞍点
- 梯度消失（爆炸）问题

下面是几个基本概念
### 鞍点
![](https://gitee.com/feynmanrp/img/raw/master/img/20220104220036.png)

由于每一维上为局部最低点或局部最高点的概率几乎为1/2.当维数D很大时，此时为极小值的概率为$(\frac{1}{2})^D$是很小的。所有在高维空间中，难点变成了如何逃离鞍点。

### 平坦最小值
一个平坦最小值的领域内，所有点对应的训练损失都比较接近。大部分局部最小解都是等价的。

### 优化地形

![](https://gitee.com/feynmanrp/img/raw/master/img/20220104224104.png)

上图是否使用残差连接的对比，可以看出残差连接不仅可以缓解梯度消失问题，还可以使优化更加平缓，使优化效率更高

### 神经网络优化的改善方法
- 更有效的优化算法来提高优化方法的效率和稳定性
  - 动态学习率调整
  - 梯度估计修正
- 更好的参数初始化方法、数据预处理方法来提高优化效率
- 修改网络结构来得到更好的优化地形
  - 好的优化地形通常比较平滑
  - 使用ReLU激活函数、残差连接、逐层归一化
- 使用更好的超参数优化方法

## 优化算法
### 优化算法改进
#### 小批量随机梯度下降
**小批量随机梯度下降**
选取K个训练样本${\{x^{(K)},y^{(k)}\}}^K_{k=1}$,计算偏导数

$$G_t(\theta)=\frac{1}{K}\sum_{(x,y)\in S_t}\frac{\partial L(y,f(x;\theta))}{\partial \theta}$$

定义梯度

$$g_t\triangleq G_t(\theta_{t-1})$$

更新参数

$$\theta_t\gets\theta_{t-1}-\alpha g_t$$

**批量大小**

批量大小不会影响随机梯度的期望，但会影响随机梯度的方差。批量越大，随机梯度的方差越小，引入噪声也越小，训练稳定，可以设置较大学习率(根据线性缩放规则)；批量较小时，需要设置较小的学习率，否则模型不收敛
![](https://gitee.com/feynmanrp/img/raw/master/img/20220105103231.png)
批量越大，每次迭代的损失变化越明显，效率越高；批量越小，随机性越强，模型的泛化能力往往越好。一般来说，当样本数量足够多时，这个时候泛化性能不是主要考虑的点，这时候批量大小越大越好（把GPU用满）；当样本数量不够的情况下，使用小批量会使模型的泛化性能较好。

### 动态学习率
![](https://gitee.com/feynmanrp/img/raw/master/img/20220105104117.png)
如图所示，学习率过大会导致不收敛，学习率过小会导致收敛太慢，通常合适的学习率是自适应的。

**逆时衰减**

$$\alpha_t=\alpha_0\frac{1}{1+\beta×t}$$

**指数衰减**

$$\alpha_t=\alpha_0\beta^t$$

**自然指数衰减**

$$\alpha_t=\alpha_0exp(-\beta×t)$$

**余弦衰减**

$$\alpha_t=\frac{1}{2}\alpha_0(1+cos((\frac{t\pi}{T})))$$

![](https://gitee.com/feynmanrp/img/raw/master/img/20220105111126.png)

**周期性学习率调整**
- 三角循环学习率
- 带热重启的余弦衰减

带周期的学习率有利于跳出尖锐的局部极小点，到达平坦最小值

#### 其他学习率调整方法
- 增大批量大小（可以变向减少学习率）
- 学习率预热：在学习最开始使用小的学习率，使优化更稳定
  
#### 自适应学习率
- Adagrad
- RMSprop
- Adadelta

### 梯度估计修正
#### 动量法
用之前累积动量来替代真正的梯度,负梯度的“加权移动平均”

$$\Delta\theta_t=\rho\Delta\theta_{t-1}-\alpha g_t$$

$$=-\alpha\sum_{\tau=1}^t\rho^{t-\tau}g_\tau$$

#### Adam算法 动量法+RMSprop

#### 梯度截断
梯度截断是一种比较简单的启发式方法，把梯度的模限定在一个区间，当梯度的模小于或大于这个区间时就进行截断。

按值截断

$$g_t=max(min(g_t,b),a)$$

按模截断

$$g_t=\frac{b}{||g_t||}g_t$$

## 其它优化
### 参数初始化
#### 随机初始化
- Gaussian分布初始化：参数从一个固定均值和固定方差的高斯分布进行随机初始化
- 均匀分布初始化：参数可以在区间$[-r,r]$内采用均匀分布进行初始化

### 数据预处理
如果机器学习算法在缩放全部或部分特征后不影响学习和预测
#### 常用的规范化方法
- 最小最大值规范化

  $$\hat{x}^{(n)}=\frac{x^{(n)}-min_n(x^{(n)})}{max_n{x^{(n)}}-min_n(x^{(n)})}$$

- 标准化

$$\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}$$

$$\mu\frac{1}{N}\sum_{n=1}^{N}x^{(n)}$$

$$\sigma^2=\frac{1}{N}\sum_{n=1}^{N}(x^{(n)}-\mu)^2$$

- PCA:去掉参数的相关性，也叫白化

![](https://gitee.com/feynmanrp/img/raw/master/img/20220106191040.png)
### 逐层规范化
- 目的
  - 更好的尺度不不变形
  - 更平滑的优化地形

#### 批量规范化
深度神经网络的第l层为

$$a^{(l)}=f{(z^{(l)})}=f(Wa^{(l-1)}+b)$$

给定一个包含K个样本的小批量样本集合，计算均值和方差

$$\mu_B=\frac{1}{K}\sum_{k=1}^{K}z^{(k,l)}$$

$$\sigma^2_B=\frac{1}{K}\sum_{k=1}^{K}(z^{(k,l)}-\mu_B)\odot(z^{(k,l)}-\mu_B)$$

批量规范化

$$\hat{z}^{(l)}\frac{z^{(l)}-\mu_B}{\sqrt{\sigma^2_B+\epsilon}}\odot\gamma+\beta$$

优点：提高优化效率，隐形的正则化方法

缺点：小批量样本的数量不能太小，无法应用到循环神经网络

#### 层规范化
与批量规范化类似，但所求的均值方差的目标变为第$l$层神经元的净输入$z^{(l)}$


### 超参数优化
**超参数**：层数、每层神经元个数、激活函数、学习率、正则化系数、mini-batch大小
#### 超参数优化
**网格搜索**

假设总共有K个超参数，第k个超参数的可以取$m_k$个值，那么这些超参数可以有$m_1*m_2*...*m_k$个组合。其中如果参数是连续的，可以将参数离散化，选择几个“经验”值。网格搜索类似于暴力枚举

**随机搜索**

由于有些超参数对于神经网络的结构很小，导致网络搜索的效率较低（因为那一维上的取之变化是多余的），这时候可以采取随机搜索的方法

其他超参数优化：贝叶斯优化、动态资源分配、神经架构搜索
## 网络正则化
### 网络正则化
神经网络特点
- 拟合能力强
- 过度参数化

由于神经网络具有很强的拟合能力（通用近似定理）和庞大的参数，有时会导致模型的泛化能力不尽人意。一般传统机器学习认为，模型越复杂，越容易导致泛化性能下降。但有文章指出这个想法不能直接照搬到神经网络中，实验证明，神经网络倾向于先记住一般性规律，最后才回去记住不那么一般的数据。

通常的正则化有两种，一个是增加优化约束，一个是干扰过程

**早停法**

我们使用一个验证集（Validation Dataset）来测试每一次迭代的参数在验证集上是否最优。如果在验证集上的错误率不再下降，就停止迭代
![](https://gitee.com/feynmanrp/img/raw/master/img/20220106204436.png)

**权重衰减**
通过限制权重的取值范围来干扰优化过程，降低模型能力，在每次参数更新时，引入一个衰减系数$\beta$,一般取值比较小，比如0.0005

$$\theta_t=(1-\beta)\theta_{t-1}-\alpha g_t$$

### l1和l2正则化
其优化问题可以写为

$$\theta^*=arg min\frac{1}{N}\sum_{n=1}^NL(y^{(n)},f(x^{(n)};\theta))+\lambda l_p(\theta)$$

$$l_1:\sum_i|\theta_i|\leq1$$

$$l_2:\sum_i(\theta_i)^2\leq1$$

其中，$l_p$为范数函数，有$l_1$,$l_2$范数，$\lambda$为正则化系数。

近似等价于

$$\theta^*=arg min\frac{1}{N}\sum_{n=1}^NL(y^{(n)},f(x^{(n)};\theta))$$

$$s.t.\quad l_p(\theta)\leq1$$

在标准的随机梯度下降中，$l_2$正则化和权重衰减效果类似，在较为复杂的优化方法中,并不等价（为什么？）
### 暂退法（Dropout）
神经层

$$y=f(Wx+b)$$

引入一个掩蔽函数使得

$$y=f(Wmask(x)+b)$$

$$mask(x)=m\odot x$$

其中$m\in {\{0,1\}}^D$，通过以概率为$p$的贝努力分布随机生成
集成学习的解释：每做一次暂退，相当于从原始的网络中采样得到一个字网络。如果一个神经元有$n$个神经元，那么总共可以采样出$2^n$个子网络
### 数据增强

通过人工构造新的样本来增强训练数据的数量和多样性

增强方法

图像数据

- 旋转：将图像按顺时针或逆时针方向随机旋转一定角度
- 翻转：将图像沿水平或垂直方法随机翻转一定角度
- 缩放：将图像放大或缩小一定比例
- 平移：将图像延水平或垂直方向平移一定步长
- 加噪声：加入随机噪声

文本数据
- 词汇替换
- 回译
- 随机编辑噪声

标签平滑：引入一个噪声对标签进行平滑，即假设样本以$\epsilon$的概率为其他类。平滑后的标签为

$$\hat{y}=[\frac{\epsilon}{K-1},...,\frac{\epsilon}{K-1},1-\epsilon,\frac{\epsilon}{K-1},...,\frac{\epsilon}{K-1}]$$

