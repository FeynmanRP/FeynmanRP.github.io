> First version on 2021-1-8
>
>Last version on 2022-1-8
>
>Author :FeynmanRP
# 无监督学习
无监督学习顾名思义就是不依靠人为监督，不打标签。让机器自己发觉数据的特征。主要有三个方向，一类是横向的特征学习，一类是纵向的聚类，还有一类是发掘底层数据分布的密度估计
## 聚类
将样本集合中相似的样本分配到相同的类/簇，不相似的样本分配到不同的类/簇，使得类内样本间距较小而类间样本间距较大


### 核心概念

样本间距离/相似性：
- L1、L2距离
- 余弦距离
- 相关系数
- 汉明距离

类/簇：没有一个严格的定义，可以理解为一组相似的样本。理解不同，聚类结果也不同

类内间距：
- 样本间平均距离
- 样本间最大距离
类间距离：
- 样本间最短距离
- 样本均值距离

常见聚类任务：
- 图像分割
- 文本聚类
- 社交网络分析

### 聚类效果评估
有外部参考聚类结果
- jaccard系数
- FM指数
- Rand指数

无外部参考聚类结果
- DB指数
- Dunn指数

## K均值
生成步骤：

1. 设置聚类个数k，随机初始k个点作为类中心
2. 此时根据中心点位置把数据分为k块
3. 重新计算类中心点，反复之下2-3步，直至收敛（类中心不再变化）

优点：实现简单，时间复杂度低$O(N)$

缺点：K值选择，主要适合凸集，初始值影响较大

## 层次聚类

聚合或分裂操作

优缺点
- 简单，理解容易
- 合并点/分裂点选择不太容易
- 合并/分类的操作不能进行撤销
- 大数据集不太合适
- 执行效率较低$O(TN^2)$

## 特征学习

### 主成分分析
数据的原始特征可能存在维度过高、冗余的问题，我们可以使用降维解决

主成分分析法是一种最常用的数据降维方法

线性投影

$$z=W^Tx$$

并满足

$$W^TW=I$$

优化准则：
最大投影方法：使得在转换后的空间中数据的方差最大，尽可能多地保留原数据的信息
最小重构误差（等价于上准则）

### 自编码器
重构错误


