> First version on 2021-12-20
>
>Last version on 2021-12-27
>
>Author :FeynmanRP

# 前馈神经网络
- [前馈神经网络](#前馈神经网络)
  - [神经元](#神经元)
    - [激活函数的性质：](#激活函数的性质)
    - [常用的激活函数](#常用的激活函数)
  - [神经网络](#神经网络)
  - [前馈神经网络](#前馈神经网络-1)
  - [反向传播算法](#反向传播算法)
    - [计算梯度](#计算梯度)
  - [计算图与自动微分](#计算图与自动微分)
    - [如何实现？（狗头）](#如何实现狗头)
  - [优化问题（概述）](#优化问题概述)
## 神经元

生物神经元：单个神经元细胞只有两种状态：兴奋和抑制

人工神经元：
![](https://gitee.com/feynmanrp/img/raw/master/img/20211220212358.png)
对于一个人工神经元，可以把它当简单的线性模型。人工神经元定义可以看出，它包含两方面内容：一个是收集其他神经元传过来的信息，体现在$z=w^Tx+b$；另一部分把收集来的神经元信息经过一个非线性函数映射到激活状态，体现在$a=f(z)$

### 激活函数的性质：

1.连续并可导（允许少数点上不可导）的非线性函数，这样我们就能通过数值优化的方法来学习网络参数

2.激活函数及其导函数要尽可能的简单，这样有利于提高网络计算效率

3.激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。

### 常用的激活函数
**S形函数**
![](https://gitee.com/feynmanrp/img/raw/master/img/20211220215611.png)

Logistic:

$$\sigma(x)=\frac{1}{1+exp(-x)}$$

Tanh:

$$tanh(x)=\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$$

且$tanh(x)=2\sigma(2x)-1$

**斜坡函数**
![](https://gitee.com/feynmanrp/img/raw/master/img/20211220221045.png)
ReLU:

$$ReLU(x)=max(0,x)$$

优点：

1.计算上更加高效

2.生物学合理性（单侧抑制，宽兴奋边界）

3.在一定程度上缓解梯度消失问题

**复合函数**
![](https://gitee.com/feynmanrp/img/raw/master/img/20211220222128.png)

Swish:

$$swish(x)=x\sigma(\beta x)$$

是一种自门控激活函数，通过变化$\beta$使得激活函数介于线性函数与ReLU之间 

## 神经网络
20世纪80年代后期，最流行的一种连接主义模型是分布式并行处理网络，其有3个主要特征：

1.信息表示是分布式的（非局部的）

2.记忆和知识是储存在单元之间的连接上

3.通过逐渐改变单元之间的连接强度来学习新的知识

## 前馈神经网络
![](https://gitee.com/feynmanrp/img/raw/master/img/20211220224533.png)
特点：

1.各神经元分别属于不同的层，层内无连接

2.相邻两层之间的神经元全部两两连接

3.整个网络中无反馈，信号从输入层单向传播，可用一个有向无环图表示

约定记号：
![](https://gitee.com/feynmanrp/img/raw/master/img/20211223162207.png)

信息传播公式：

$$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$$

$$a^{(l)}=f_l(z^{(l)})$$

通用近似定理：
![](https://gitee.com/feynmanrp/img/raw/master/img/20211227110955.png)
根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数

## 反向传播算法
### 计算梯度
结构化风险函数

$$R(W,b)=\frac{1}{N}\sum_{n=1}^NL(y^{(n)},\hat{y}^{(n)})+\frac{1}{2}\lambda||W||_F^2$$

求偏导

$$\frac{\partial R(W,b)}{\partial W^{(l)}} \quad \frac{\partial R(W,b)}{\partial b^{(l)}}$$

剩下的以后补，略略略
## 计算图与自动微分

自动微分是利用链式法则来自动计算一个复合函数的梯度，我们可以使用计算图来计算微分，比如

$$f(x;w,b)=\frac{1}{exp(-(wx+b))+1}$$

如图所示
![](https://gitee.com/feynmanrp/img/raw/master/img/20211227160732.png)
汇总成表
![](https://gitee.com/feynmanrp/img/raw/master/img/20211227161543.png)

$f(x;w,b)$关于参数$w$和$b$的导数可以通过计算图上的路径上的所有导数连乘得到
![](https://gitee.com/feynmanrp/img/raw/master/img/20211227171954.png)

静态计算图和动态计算图

静态计算图：是在编译时构建计算图，计算图构建好之后在程序运行时不能改变，如Theano和Tensorflow

动态计算图:动态计算是在程序运行时动态构建，如DyNet,Chainer和PyTorch

### 如何实现？（狗头）
![](https://gitee.com/feynmanrp/img/raw/master/img/20211227172748.png)

## 优化问题（概述）

1.非凸优化问题

一般来说，神经网络参数学习都是非凸优化的，所以很难达到全局最优

2.梯度消失问题

当神经网络很深的时候下层的很多参数都为0，会导致参数更新不动

3.参数过多，影响训练，参数解释性较差

**需求：**

计算资源要大

数据要多

算法效率要好（收敛快）



