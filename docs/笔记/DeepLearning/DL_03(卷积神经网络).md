> First version on 2021-12-27
>
>Last version on 2021-12-31
>
>Author :FeynmanRP

- [卷积神经网络](#卷积神经网络)
  - [卷积(Filter)](#卷积filter)
  - [卷积的作用](#卷积的作用)
    - [近似微分](#近似微分)
    - [低通滤波器/高通滤波器](#低通滤波器高通滤波器)
  - [卷积扩展](#卷积扩展)
  - [二维卷积](#二维卷积)
    - [卷积作为特征提取器](#卷积作为特征提取器)
  - [卷积神经网络（CNN）](#卷积神经网络cnn)
    - [互相关](#互相关)
    - [多个卷积核](#多个卷积核)
    - [卷积层](#卷积层)
    - [汇集层（Pooling Layers）](#汇集层pooling-layers)
    - [卷积网络结构](#卷积网络结构)
  - [典型的卷积网络](#典型的卷积网络)
    - [LeNet-5](#lenet-5)
    - [AlexNet](#alexnet)
    - [Inception(GoogLeNet)](#inceptiongooglenet)
    - [ResNet(残差网络)](#resnet残差网络)
    - [卷积网络的应用](#卷积网络的应用)
# 卷积神经网络

书接上回，我们学习了全连接的前馈神经网络，但是其中存在一些问题。如下：

1.权重矩阵的参数非常多，当神经元数量很大的时候，学习其中的参数是很困难的

2.局部不变性特征

  - 自然图像中的物体都具有局部不变形特征，要求尺度缩放、平移、旋转等操作不影响其语义信息
  - 全连接前馈网络很难提取这些局部不变特征

所以引出本章要介绍的内容：卷积神经网络（Convolutional Neural Networks,CNN）
- 一种前馈神经网络
- 受生物学上感受野（Receptive Field）的机制而提出的
  - 在视觉神经系统中，一个神经元的感受野是指网络上的特定区域，只有这个区域内的刺激才能够激活该神经元

卷积神经网络有三个结构上的特性：
- 局部连接
- 权值共享
- 空间或时间上的次采样
  
## 卷积(Filter)
卷积经常用在信号处理中，用于计算信号的延时累积。

假设一个信号发射器每个时刻t产生一个型号$x_t$,其信息的衰减率为$w_k$，即在k-1个时间步长后，信息为原来的$w-k$倍

假设有$w_1,w_2,w_3...w_K$

时刻t收到的信号$y_t$为当前时刻产生的信息和以前时刻延迟信息的累积，表示为：

$$y_t=\sum_{k=1}^Kw_k·x_{t-k+1}$$

其中$w_k$被称为滤波器或卷积核

一般的，也表示成$y=x*w$,其中$*$为卷积符号

例如：
当$w=[-1,0,1],x=[1,1,2,-1,1,-2,1]$时，有输出$y=[-1,2,1,1,0]$

![](https://gitee.com/feynmanrp/img/raw/master/img/20211230153019.png)
对于一维卷积来说，输入信号$N$，卷积核$K$，输出个数为$N-K+1$

## 卷积的作用

### 近似微分 
当令滤波器$w=[1/2,0,-1/2]$时，可以近似信号序列的一阶微分

$$x'(t)=\frac{x(t+1)-x(t-1)}{2}$$

当令录波器$w=[1,-2,1]$时，可以近似信号序列的二阶微分

$$x''(t)=x(t+1)+x(t-1)-2x(t)$$

### 低通滤波器/高通滤波器
滤波器$w[1/3,1/3,1/3]$可以检测信号序列中的低频信号

滤波器$w=[1,-2,1]$可以检测信号序列中的高频信息

## 卷积扩展
引入滤波器的滑动步长S和零填充P

<table>
    <tr>
        <td ><img src="https://gitee.com/feynmanrp/img/raw/master/img/20211230160544.png" ></td>
        <td ><img src="https://gitee.com/feynmanrp/img/raw/master/img/20211230160600.png"  ></td>
    </tr>
</table>
卷积的结果按输出长度不同可以分为三类：

窄卷积(narrow convolution)：步长$s = 1$，两端不补零$p = 0$，卷积后输出长度为$M-K+ 1$。

宽卷积(wide convolution)：步长$s = 1$，两端补零$p = K − 1$，卷积后输出长度$M+K− 1$。

等宽卷积(equal-width convolution)：步长$s = 1$，两端补零$p = (K − 1)/2$，卷积后输出长度M。

## 二维卷积

定义如下：

$$y_{ij}=\sum_{u=1}^U\sum_{v=1}^Vw_{uv}x_{i-u+1,j-v+1}$$

![](https://gitee.com/feynmanrp/img/raw/master/img/20211230163420.png)

### 卷积作为特征提取器
我们可以人工构造卷积核来提取图像的不同信息。
下图中最上面的滤波器是高斯滤波器，可以用来对图像进行平滑去噪；中间和最下面的滤波器可以用来提取边缘特征

![](https://gitee.com/feynmanrp/img/raw/master/img/20211230164101.png)

将卷积应用到神经网络中，我们希望网络自动学习卷积核$w$是可学习的参数，不用再人工设计。

## 卷积神经网络（CNN）
![](https://gitee.com/feynmanrp/img/raw/master/img/20211230165430.png)
相比于全连接神经网络，卷积神经网络改写成

$$h^{(l+1)}=f(w*h^{(l)}+b)$$

### 互相关

计算卷积需要进行卷积核翻转。但是我们做卷积操作的目标是提取特征，所以翻转是不必要的，我们称其为互相关。（但基于习惯，我们仍称其为卷积）

$$y_{ij}=\sum_{u=1}^U\sum_{v=1}^Vw_{uv}x_{i+u-1,j+v-1}$$

### 多个卷积核
因为一个卷积核能提取到的特征有限，所以一个自然的想法就是使用多个卷积核

特征映射（Feature Map）：图像经过卷积后得到的特征

### 卷积层
- 输入：D个特征映射M×N×D
- 输出：P个特征映射M'×N'×P

下图显示的是第p个卷积核对输入特征映射的操作：经过求和，加上偏置，再非线性映射后得到第P个输出映射$Y^p$。每一个输出的特征映射和输入映射都是全连接关系。假设每个卷积核的大小为U\*V，那么共需要：P\*D\*(U\*V)+P个参数

![](https://gitee.com/feynmanrp/img/raw/master/img/20211230192750.png)
公式为：

$$Z^p=W^p⊗X+b^p=\sum_{d=1}^DW^{p,d}⊗X^d+b^p$$

$$Y^p=f(Z^p)$$

具体例子如下

![](https://gitee.com/feynmanrp/img/raw/master/img/20211230194700.png)

我们可以把这种操作更加形象的表示成3维结构：
![](https://gitee.com/feynmanrp/img/raw/master/img/20211230194959.png)

### 汇集层（Pooling Layers）
卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少，由此引入汇聚层，来减少神经元数量。常用的汇聚有最大汇集，平均汇聚。

### 卷积网络结构
卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成
![](https://gitee.com/feynmanrp/img/raw/master/img/20211230215204.png)
一个卷积块为连续M个卷积层和b个汇聚层（M通常设置为2～5，b为0或1）。一个卷积网络中可以堆叠N个连续的卷积块，然后在接着K个全连接层（N的取值区间比较大，比如1～100或更大；K一般为0～2）

由于卷积的性质，在网络较浅的情况下，神经元只能提取到局部的性质。随着层数的增加，神经元可以学到更高级的特征

![](https://gitee.com/feynmanrp/img/raw/master/img/20211230221121.png)

## 典型的卷积网络

### LeNet-5
基于 LeNet-5 的手写数字识别系统在 90 年代被美国很多银行使用，用来识别支票上面的手写数字。LeNet-5 共有 7 层。
![](https://gitee.com/feynmanrp/img/raw/master/img/20211230231438.png)

### AlexNet
首个现代深度卷积网络模型
- 使用GPU进行并行训练，采用了ReLU作为非线性激活函数，使用Dropout防止过拟合，使用数据增强
  
5个卷积层、3个汇聚层和3个全连接层
![](https://gitee.com/feynmanrp/img/raw/master/img/20211230232546.png)

### Inception(GoogLeNet)
Inception网络是由多个inception模块和少量的汇聚层堆叠而成。
![](https://gitee.com/feynmanrp/img/raw/master/img/20211230233250.png)

在Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模块

Inception模块同时使用1×1、3×3、5×5等不同大小的卷积核，并将得到的特征映射在深度上拼接（堆叠）起来作为输出特征映射

### ResNet(残差网络)

残差网络是通过给非线性的卷积层增加直连边的方式来提高信息的传播效率

假设在一个深度网络中，我们期望一个非线性单元（可以为一层或多层的卷积层）$f(x,\theta)$去逼近一个目标函数为$h(x)$

将目标函数拆分成两部分：恒等函数和残差函数

$$h(x)=x+(h(x)-x)$$

优势：导数为1左右，不容易出现梯度消失现象

### 卷积网络的应用
AlphaGo、目标检测、野外OCR、文本数据等