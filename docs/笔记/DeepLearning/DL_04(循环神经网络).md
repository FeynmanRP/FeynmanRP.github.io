> First version on 2021-12-31
>
>Last version on 2022-1-1
>
>Author :FeynmanRP

- [循环神经网络](#循环神经网络)
  - [如何给网络增加记忆能力](#如何给网络增加记忆能力)
    - [时延神经网络](#时延神经网络)
    - [自回归模型](#自回归模型)
    - [由外部输入的非线性自回归模型](#由外部输入的非线性自回归模型)
  - [循环神经网络](#循环神经网络-1)
  - [参数学习与长程依赖问题](#参数学习与长程依赖问题)
    - [定义损失](#定义损失)
    - [计算梯度](#计算梯度)
  - [如何解决长程依赖问题](#如何解决长程依赖问题)
  - [常见的循环网络模型](#常见的循环网络模型)
    - [门控循环单元GRU](#门控循环单元gru)
    - [长短期记忆网LSTM](#长短期记忆网lstm)
# 循环神经网络
> 前馈神经网络输入输出的维数都是固定的，不能任意改变，无法处理变长的序列数据；此外，前馈神经网络每次输入都是独立的也就是说每次网络的输出只依赖于当前的输入

## 如何给网络增加记忆能力

### 时延神经网络
建立一个额外的延时单元，用来储存网络的历史信息（包括输入、输出、隐藏层）

$$h^{(l)}_{t}=f(h^{(l-1)}_{t},h^{(l-1)}_{t-1},...,h^{(l)}_{t-K})$$

### 自回归模型

一类时间序列模型，用变量$y_t$的历史信息来预测自己

$$y_t=w_0+\sum_{k=1}^Kw_ky_{t-k}+\epsilon_t$$

$\epsilon_t～N(0,\sigma^2)$为第t个时刻的噪声，该模型结果只与自己历史信息有关

### 由外部输入的非线性自回归模型

$$y_t=f(x_t,x_{t-1},...,x_{t-K_x},y_{t-1},y_{t-2},...,y_{t-K_y})$$

其中f(·)表示非线性函数，可以是一个前馈网络。K_x和K_y为超参数
![](https://gitee.com/feynmanrp/img/raw/master/img/20220101143806.png)

## 循环神经网络

循环神经网络通过使用带自反馈的神经元，能够处理任意长度的时序数据

$$h_t=f(Uh_{t-1},Wx_t+b)$$

根据循环神经网络的通用近似定理，一个完全连接的循环网络是任何非线性动力系统的近似器；根据图灵完备性证明，一个完全连接的循环神经网络可以近似解决所有的可计算问题。（这里没有给出证明，留有个印象就行）

循环神经网络作用：
- 输入输出映射（本章主要关注情况）
- 储存器


## 参数学习与长程依赖问题
### 定义损失
时刻t的瞬时损失函数为：

$$L_t=L(y_t,g(h_t))$$

总损失函数：

$$L=\sum_{t=1}^{T}L_t$$

![](https://gitee.com/feynmanrp/img/raw/master/img/20220101150503.png)

### 计算梯度

$$z_t=Uh_{t-1}+Wx_t+b$$

$$h_t=f(z_t)$$

求偏导

$$\frac{\partial L}{\partial U}=\sum_{t=1}^T\frac{\partial L_t}{\partial U}$$

$$\frac{\partial L_t}{\partial U}=\sum_{k=1}^{t}\frac{\partial L_t}{\partial z_k}h^T_{k-1}$$

$$=\sum_{k=1}^{t}\delta_{t,k}h^T_{k-1}$$

$\delta_{t,k}$为第t时刻的损失对第k步隐藏神经元第净输入$z_k$的导数

$$\delta_{t,k}=\frac{\partial L_t}{\partial z_k}$$

$$=\frac{\partial h_k}{\partial z_k} \frac{\partial z_{k+1}}{\partial h_k}\frac{\partial L_t}{\partial z_{k+1}}$$

$$=diag(f'(z_k))U^T\delta_{t,k+1}$$

称为随时间反向传播算法（BPTT）

$$\delta_{t,k}=\prod_{\tau=k}^{t-1}(diag(f'(z_\tau))U^T)\delta_{t,t}$$

使用$\gamma$代替$diag(f'(z_\tau))U^T$后得

$$\delta_{t,k}\approx \gamma^{t-k}\delta_{t,t}$$

此时，无论$\gamma$大于1还是小于1，只要当$t-k$趋向于$∞$时，总会产生梯度爆炸或梯度消失的问题，换言之只要$t$与$k$的距离足够大，就会出现梯度爆炸或梯度消失，使得网络只能学到短周期短依赖关系。我们把这个问题称为长程依赖问题。

## 如何解决长程依赖问题

1.循环边改为线性依赖关系

$$h_t=h_{t-1}+g(x_t;\theta)$$

2.增加非线性

$$h_t=h_{t-1}+g(x_t,h_{t-1};\theta)$$

## 常见的循环网络模型
门控机制：

控制信息的累积速度，包括有选择地加入新的信息，并有选择地遗忘之前累积的信息。

### 门控循环单元GRU
![](https://gitee.com/feynmanrp/img/raw/master/img/20220101232206.png)
### 长短期记忆网LSTM
![](https://gitee.com/feynmanrp/img/raw/master/img/20220101232251.png)



