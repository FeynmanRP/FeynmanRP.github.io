> First version on 2022-6-11
>
>Last version on 2022-6-13
>
>Author :FeynmanRP


# 自然语言处理
## 1）绪论

### 一些概念
* NLP定义:随着社会发展进程自然产生并发展演化的用于人类进行日常交流与思考的符号系统
* 自然语言特点：
  * 形式灵活多变
  * 规则无法完全覆盖现象
  * 内容不断发展变化
* 自然语言处理:一门旨在实现让计算机模拟人类对自然语言进行智能化处理的技术
* 自然语言处理的主要过程：词法分析、句法分析、语义分析、篇章分析

### 自然语言处理

#### 研究内容以及层次划分
**应用层研究**

机器翻译：实现一种语言到另一种语言的自动翻译

信息检索：也称情报检索，就是利用计算机系统从大量文档中找到符合用户需要的相关信息

自动文摘：将原文档的主要内容或从某方面的信息自动提取出来，并形成原文档的摘要或缩写

问答系统：通过计算机系统对人提出的问题的理解，利用自动推理等手段，在有关知识资源中自动求解答案并做出相应的回答。问答技术有时和语言技术和多模态输入输出技术，以及人机交互技术等相结合，构成人机对话系统

文档分类：文档分类也叫文本自动分类或信息分类，其目的就是利用计算机系统对大量的文档按照一定的分类标准实现自动分类

信息抽取：从自然语言文本中抽取出特定的信息，以便将文本的关键信息机构化或用作其他用途

**应用基础研究**
  
词法分析：对输入的自然语言字符串进行的词一级处理

句法分析：对输入的自然语言字符串进行句子级的分析，基本任务包含确定句法结构、确定词汇之间的依存关系

语义分析：语义分析指建立语言与客观世界知识内涵之间的可计算模型，这至今是个未能解决的难题。当前的语义分析限定在从某个细小角度做出语义内涵层面的区分或鉴别

**基础研究**

语言模型：我们目前倾向于认为语言本身并不是由随机抽取的一些单词组成的序列，词与词之间是有联系的，语言模型即是刻画真实语言世界中词与词之间联系规律的模型。目前最常用的语言模型是N元语法模型，它基于概率理论反映任意一个字符串作为一个句子出现的概率

语料库建设：收集、整理、标注出可用于某项语言学理论研究或经验主义方法研究的自然语言文本数据集

语言知识库建设：结构化的语义词典/语言规则库/语言概念库，即给机器准备的、供它参考的知识库

#### 面临主要困难
* 歧义
  * 分词歧义
  * 短语歧义
  * 词义歧义
  * 语用歧义

* 未知语言现象（病构）
  * 新的词汇
  * 新的词义
  * 新的句子结构

#### 两大研究方法
理性主义方法：通常通过一些特殊的语句或语言现象的研究来得到对人的语言能力的认识，而这些而这些语句和语言现象在实际的应用中并不常见。

* 基本思路：基于规则的分析方法建立符号处理系统
* 规则库开发：N+N->NP
* 词典标注：#工作，N(uc)，V
* 推导算法设计：归约、推导、歧义消解方法
* 知识库+推理系统->NLP系统  理论基础：Chomsky的文法理论

经验主义：偏重于对大规模语言数据中人们所实际使用的普通语句的统计。

* 基本思路：基于大规模真实语料建立计算方法
* 大规模真实数据的收集、标注：真实性、代表性、标注信息
* 统计模型建立：模型的复杂性、有效性、参数训练方法
* 语料库+统计模型->NLP系统 理论基础：统计学、信息论、机器学习

## 2）数学基础
### 最大熵模型
原理：最大熵原理指出，对于一个随机事件的概率分布进行预测时，预测应当满足全部已知的约束，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小，因此得到的概率分布的熵是最大。

满足条件

* 满足已知信息（约束条件）
* 不做任何未知假设（剩下的等概率）

**信息熵**：一个系统的信息熵其实就是系统中每一个事件的概率乘以log概率，然后把所有事件相加后取负数，数学表示如下。

$$H(x)=-\sum_{x\in X}p(x)log_{2}p(x)$$

熵又称为自信息(self-information)，表示信息源X每发一个符号所提供的平均信息量。熵也可以被视为描述一个随机变量的不确定性的数量。一个随机变量的熵越大，它的不确定性越大。那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。

**联合熵**
如果X，Y是一对离散型随机变量，$X，Y～p(x,y)$,X,Y的联合熵$H(X,Y)$为：

$$H(X,Y)=-\sum_{x\in X}\sum_{y\in Y}p(x,y)log_2p(x,y)$$

联合熵实际上就描述一对随机变量平均所需要的信息量。

**条件熵**
给定随机变量X的情况下，随机变量Y的条件熵定义为：

$$H(Y|X)=\sum_{x\in X}p(x)H(Y|X=x)$$

$$=\sum_{x\in X}p(x)[-\sum_{y\in Y}p(y|x)log_{2} p(y|x)]$$

条件熵和联合熵的关系：

$$H(X,Y)=H(X)+H(Y|X)$$

**互信息**

如果(X,Y)~p(x,y),X,Y之间的互信息I(X;Y)定义为：

$$I(X;Y)=H(X)-H(X|Y)$$

互信息I(X;Y)是在知道了Y的值以后X的不确定性的减少量，即Y的值透露了多少关于X的信息量

互信息、条件熵、联合熵关系：
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613110022.png)

### 形式语言与自动机
#### 定义
形式语言是用来精确地描述语言(包括人工语言和自然语言)及其结构的手段。形式语言学也称 代数语言学。

#### 分类
* 正则文法（3型）
* 上下文无关文法（2型）
* 上下文有关文法（1型）
* 无约束文法（0型）

显然，每一个正则文法都是上下文无关文 法，每一个上下无关文法都是上下文有关文法， 而每一个上下文有关文法都是0型文法

#### 如何判断文法类型
**3型文法**

* 左边只有一个字符且为非终结符。
* 右边最多只能有两个字符。 当右边有两个字符时，必须一个为终结符，一个为非终结符。 当右边只有一个字符时，此字符必须为终结符。
* 所有右边为两个字符的产生式，终结符和非终结符位置为终结符+非终结符或者非 终结符+终结符。且一个文法中终结符和非终结符位置必须相同。

**2型文法**

* 同3型文法第一条。
* 所有产生式的右边可以含有有限个终结符和非终结符。就是终结符和非终结符的数 量没有限制。只要是有限个即可。

**1型文法**
* 所有产生式左边至少有一个非终结符。
* 同2型文法第二条。

**0型文法**
* 当文法与前面的类型都不符合时，则为0型文法。

#### 语言和识别器对应关系
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613113400.png)

### 隐马尔可夫模型与条件随机场
描写：该模型是一个双重随机过程，我们不知 道具体的状态序列，只知道状态转移的概率， 即模型的状态转换过程是不可观察的(隐蔽 的)，而可观察事件的随机过程是隐蔽状态转 换过程的随机函数。

#### 举例理解

1）N 个袋子，每个袋子中有 M 种不同颜色的球。一 实验员根据某一概率分布选择一个袋子，然后根据袋子中 不同颜色球的概率分布随机取出一个球，并报告该球的颜 色。对局外人:可观察的过程是不同颜色球的序列，而袋 子的序列是不可观察的。每只袋子对应HMM中的一个状态; 球的颜色对应于 HMM 中状态的输出。
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613125338.png)

2）我老公现在在北京工作，而我还在杭州工作。每天下班之后，我会根 据天气情况有相应的活动:或是去商场购物，或是去公园散步，或是 回家收拾房间。我们有时候会通电话，我会告诉他我这几天做了什么， 而他则要通过我的行为猜测这几天对应的天气最有可能是什么样子的。

以上就是一个简单的 HMM，天气状况属于状态序列，而我的行为则属于观 测序列。天气状况的转换是一个马尔可夫序列。而根据天气的不同，有相对 应的概率产生不同的行为。在这里，为了简化，把天气情况简单归结为晴天 和雨天两种情况。雨天，我选择去散步，购物，收拾的概率分别是0.1，0.4， 0.5， 而如果是晴天，我选择去散步，购物，收拾的概率分别是0.6，0.3， 0.1。而天气的转换情况如下:这一天下雨，则下一天依然下雨的概率是0.7， 而转换成晴天的概率是0.3;这一天是晴天，则下一天依然是晴天的概率是 0.6，而转换成雨天的概率是0.4. 同时还存在一个初始概率，也就是第一天下 雨的概率是0.6， 晴天的概率是0.4.

![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613130016.png)

#### 隐马尔可夫模型

![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613131553.png)

#### 3个重要问题及解决方法
书接例子2，我们的重点是要了解并解决HMM的三个问题。

问题1，估计问题:已知整个模型，我告诉我他，连续三天，我下班后做的事情分别是:散步，购物，收拾。那么，根据模型，计算产生这些行为的概率是多少。

问题2，序列问题:同样知晓这个模型，同样是这三件事，我要他猜，这三天我下班后杭州的天气是怎么样的。这三天怎么样的天气才最有可能让我做这样的事情。

问题3，训练问题:最复杂的，我只告诉他这三天我分别做了这三件事，而其他什么信息我都没有。我要他建立一个模型，晴雨转换概率，第一天天气情况 的概率分布，根据天气情况她选择做某事的概率分布。(惨绝人寰)
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613134913.png)

解决：

* 如何快速计算概率$p(O|\mu)$?前向或后向算法
* 如何选择“最优”的状态序列？Viterbi算法
* 如何用最大似然估计来求模型参数值？前向后向算法

## 3）语料库与语言模型
语料库：指存放语言材料的数据库

语言知识库：诸如词典、语言规则库、语义概念库等采用形式化结构描述的显性语言知识集

语言知识库相对于语料库：

* 形式更结构化
* 知识更显性
* 加工程度更高

语言模型：语言模型（Language Model）即是刻画真实语言世界中词与词之间联系规律的模型

### 语料库
语料库作用：

* 研究语言现象
  * 1：研究语法现象的变迁
  * 2：研究不同地域的语言现象差异
  * 3：研究新词汇的出现、生命周期
  * 4：研究儿童的语言习得过程
* NLP建模
  * 1：分词模型训练
  * 2：句法分析模型训练
  * 3：机器翻译模型训练
  * 4：文本分类器训练
#### 语料库划分
按语种划分

按代表性&平衡性划分

* 平衡语料库：采集时充分考虑了语料的代表性与平衡性问题的语料库；代表性表现为各种领域的语料均被包含；平衡性表现为各种领域的语料比例得当；
* 平行语料库：采集时按照平行方式选取语料的语料库；平行方式1：指语料选取时间、对象、比例、文本数、文本长度等几乎一致；平行方式2：指多种语言语料之间的经过加工后严格对齐；

按照语料用途区分

* 通用语料库：不是为了某项特定用途而构建的语料库；
* 专用语料库：为了某项专门用途，只采集相对应的特定语料而构成的语料库；Eg. 科技语料库、医学语料库、北京地区方言语料库、……


按照语料的时间分布区分

* 共时语料库：旨在研究一个平面上的语言现象而采集的语料库
* 历时语料库：旨在对语言进行历时研究而采集的语料库

![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612111520.png)

按加工程度划分
* 生语料库:未经任何加工处理的原始语料数据
* 标注语料库:经过加工处理、标注了特定信息的语料库

#### 经典的语料库
宾夕法尼亚大学树库(Tree Bank):宾夕法尼亚大学是LDC（国际语言数据联盟）的管理机构；LDC是目前世界最大、最权威的语言数据管理与共享平台；主流的NLP研究，都与LDC有密切联系

中文树库(CTB)：Chinese Tree Bank 中汉语词汇被划分为33类，23类句法标记；由宾夕法尼亚大学开发；目前收录并发布于LDC

### 语言知识库

语言知识库作用：

* 对语义进行计算
> 例如：
> 
> Eg 1：“国王  皇帝  大臣” 3个词的相似性比较
> 
> Eg 2：“完美   不错   好 ” 3个词中那一个表示赞扬的程度最高     

* 帮助机器理解语言
> 例如：
> 
> Eg 1：让计算机从“沃尔沃XC90的发动机很费油！ ”中感知到
> 
>语义信息1： 发动机-汽车的部分-动力单元
>
>语义信息2： 费油-油耗-负面评价
>
>语义信息3： 动力单元-油耗-负面评价

#### 经典的语言知识库
WordNet：普林斯顿大学研究人员开发的机读英语词汇知识库；分为名词、动词、形容词、副词4个子库；每个子库中包含大量的synset（同义词集）；每个synset中包含一个或多个word；每个synset对应一个明确的注释；synset之间用反义关系、上下位关系、整体与部分关系、因果关系、蕴含关系、近似关系等语义关系进行标注。

HowNet：中国科学院计算机语言信息中心研究人员董振东与董强开发的机读汉语知识库；知识库中的词由词形、词性、词例、义原构成；义原是最基本的语义最小单位；义原之间用同义关系、反义关系、上下位关系、整体与部分关系、属性与宿主关系、材料与成品关系等16种语义关系进行标注。

### 语言模型
#### N-gram语言模型
只考虑前N-1个词对于后续词出现概率的影响，从而创建成的语言模型，即为N-gram模型。词语在句子中出现的概率主要依赖于其之前相邻的词

举例：基于3-gram模型，句子$W1 W2 W3 W4 … Wn$的出现概率计算简化为：

$$P (W1 W2 W3 W4 … Wn)= P (W1)* P (W2|W1)* P(W3|W2 W1)*……*P(Wn|Wn-1 Wn-2) $$


2-gram模型参数估计举例：

![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612130832.png)

![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612131710.png)

#### N-gram语言模型的平滑
平滑定义：为了让最大似然估计的结果更准确，降低高概率，提升低概率，尽量使概率分布趋于均匀，从而整体上让概率的估计结果更接近真实情况

为什么需要平滑：N-gram语言模型的参数估计中，无论采用多大的训练语料库，总是会存在大量零概率的情况，这与现实情况不符；N-gram语言模型的参数估计采用的是最大似然估计，受限于训练语料库容量的限制，存在对非零概率放大的倾向。

## 4）词法分析
词法分析：NLP系统从接受输入字符串开始到对输入字符串进行句法层面的分析之前，对输入字符串进行的词一级处理统称为词法分析（Morphological Analysis）
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612132354.png)

词法分析主要任务：

* 汉语自动分词：让计算机系统在汉语文本中的词与词之间自动加上边界标记
> eg：我们在上自然语言处理课程。->我们/在/上/自然语言处理/课程/。
* 英语形态还原
* 命名实体识别：识别文本中对实体的命名性指称，具体来讲就是识别文本中的人名、地名、机构名、时间名等专有名词
* 词性标注：根据一个词在某个特定句子中的上下文，为这个词标注正确的词性。

### 汉语自动分词
#### 主要困难
* 分词规范不明确：分词规范用于明确什么应该作为独立单元被切分出来的问题
* 切分歧义普遍：切分歧义指多种切分方式都符合形态上的规范
* 未登录词频繁：未登录词指不在词表中的词

#### 主要方法
**最大匹配法（基于词典的机械切分法）**
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612133514.png)
优点：

* 程序简单易行，开发周期短
* 仅需要很少的语言资源（词表），不需要任何词法、句法、语义资源
* 
缺点：

* 歧义消解的能力差
* 切分正确率不高

**最短路径法（基于词典的启发式切分法）**
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612134028.png)

优点：

* 切分原理符合汉语自身规律
* 需要的语言资源也不多

缺点：

* 对许多歧义字段难以区分，最短路径有多条时，选择最终的输出结果缺乏应有的标准

**最大概率法（基于N元语法模型分词法）**
**字分类法（由字构词的分词法）**

基本思想：将分词过程看作是字的分类问题。该方法认为，每个词在构造一个特定的词语时都占据着一个确定的构词位置（即词位）。假设每个字只有4个词位：词首(B)、词中(M)、词尾(E)、单独成词(S)，那么，每个字归属一特定的词位。从头至尾对句子中每一个字进行上述分类，显然就是一个经典的序列标注问题。

评价：该方法的重要优势在于，它能够平衡地看待此表词和未登录词的识别问题，文本中的词表词和未登录词都是用统一的字标注过程来实现的。在学习构架上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词识别模块，因此，大大地简化了分词系统的设计。
#### 命名实体识别
* 基于规则的方法
* 基于统计的方法

#### 词性标注
词性标注集：
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612141806.png)

方法分类：

* 基于规则的方法
  * 基于人工设计的规则(TAGGIT词性标注系统)
  * 基于自动提取的规则(基于转换规则的错误驱动的机器学习方法)
* 基于统计的方法
  * 基于HMM模型的词性标注方法
  ![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612142506.png)
  ![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612142519.png)

## 5）句法分析
句法分析：自然语言处理中的关键技术环节，限定于句子层面上的形式化分析，核心目标是确定句子的句法结构或句子词汇之间的依存关系
### 句法结构分析
句法结构分析：指对输入的单词序列（句子）判断其构成是否合乎给定的语法，并分析确定出合乎语法的句法结构

句法分析树：指用树状数据结构表示的句法结构

语法规则库如何构建？

* 基于规则的分析方法（人工CFG规则库）
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612153628.png)
* 基于统计(机器学习)的分析方法（PCFG模型、PCFG模型的各种改进）
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612153646.png)

基于语法规则库的分析算法如何设计？

* 穷举类算法（CYK分析算法）
* 动态规划类算法（Viterbi算法）

#### CYK算法（理解以下例子）
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612153936.png)
优点：

* 简单易行，执行效率高

缺点：

* 必须对文法进行范式化处理
* 无法区分歧义

#### PCFG方法
基本思路：如果有大量已标注语法结构的训练语聊，则可直接通过计算每个语法规则的使用次数，用最大似然估计方法计算PCFG规则的概率参数。多数情况下，没有可利用的标注语料，只好借助EM算法迭代算法估计PCFG的概率参数

使用方法：
* 内向算法或外向算法解决快速计算句子的句法树概率
* Viterbi算法解决最佳分析结果搜索
* 内外向算法解决参数估计


#### 浅层句法分析
浅层句法分析：也称部分句法分析，只需要求识别句子中某些结构相对简答的独立成分，如：非递归的名词短语、动词短语等。

Base NP定义：基本名词短语（Base NP）指简单的、非嵌套的名词短语，不含有其他的子语。

### 依存句法分析
依存句法分析：分析出句子中所有词汇之间的依存关系。

## 6）语义分析
语义分析：对输入的文本内容，在词法分析、句法分析的基础上，让计算机进一步识别出作者表达的含义。换一句话说：建立计算模型，让计算机能通过计算识别出语言符号与背后客观世界的真实联系

内容划分：
* 词语层面上的分析
  * 词义消歧
* 句子层面上的分析
  * 语义角色标注
* 篇幅层面上的分析
  * 共指消解
  * 话题结构分析

### 词义消歧
定义：确定一个多义词在给定上下文语境中的具体含义

方法分类

基于规则的方法：根据语言学的研究发现，人工抽取出词与词连用时的限制规则，然后用这些规则来对词义进行消歧。

基于语料库的统计学习方法：基于互信息准则，为每个需要消歧的多义词寻找一些上下文特征，这些特征能够可靠地指示该多义词在特定上下文语境中使用的那种语义，通常把这些上下文特征称作指示器

基于词典的方法：词典中词条本身的定义作为判断其语义的条件

### 词义角色标注
定义：以句子的谓词为中心，研究句子中各成分与谓词之间的关系，并用语义角色来描述它们之间的关系。

#### 常见的语义角色
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612163144.png)


#### 基本流程
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612163326.png)

## 7）文本分类
文本分类：在预定义的分类体系下，根据文本的自身特征，自动将给定文本与类别相关联的过程

一般框架：
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612163742.png)

### 文本预处理
* 中文分词
* 英文stemming
* 词性标注
* 去停用词
* 去虚词
* 识别词组

### 文本表示
* 表示模型设计
  * VSM模型（词袋模型）
  * 主题模型
  * 词嵌入模型
* 文本特征选择
  * 基于频率的特征提取法
  * 信息增益法
  * 卡方法
  * 互信息法
* 特征权重计算
  * 布尔权重法
  * 词频法
  * 倒排文档频度法
  * TF-IDF法
  * TFC法
  * ITC法
  * 熵权重
  * IF-IWF法

### 文本分类器
* 分类器训练
  * 朴素贝叶斯分类器
  * 支持向量机分类器
  * K-最近邻分类器
  * ANN分类器
  * Rocchio分类器
  * 决策树分类器
  * 组合分类器

* 分类器测试
  * 精确率
  * 召回率
  * F测度

### 文本情感分类
根据文本所表达的作者态度或情感信息，将文本划分成褒扬或贬义两种类别之一（甚至更多细分类别之一）

应用：

企业对观点挖掘和倾向性分析的需求
* 自动发现用户情感与观点
* 感知社会发展趋势
* 获取商业机会
* 在线名誉管理
* 目标导向地广告

普通用户对观点挖掘和倾向性分析的需求
* 有助于购买产品
* 有利于发现针对政治话题的观点

政府对观点挖掘和倾向性分析的需求
* 控制公众整体情绪
* 检测公共事件

## 8）信息检索
从大规模非结构化数据(通常是文本)的集合中找出满足用户信息需求的资料的过程。

一般框架
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220612173215.png)

### 基于向量空间模型的信息检索技术框架

#### VSM模型
关键点：
* 索引项选择
  * 高频出现的词项
  * 与领域相关的词项
  * 相似词项归并为同一个索引
* 权重计算
  * TF-IDF加权
* 相似度评价
  * Inner Product
  * Cosine
  * Jaccard

优点：
* 索引项的权重算法提高了检索的性能
* 部分匹配的策略使得检索的结果文档集更接近用户的检索需求
* 可以根据结果文档与查询串的相关度进行排序

缺点：
* 索引项之间被认为是相互独立，这不符合客观实际
* 一词多义、一义多词会严重影响检索效果
* 隐含语义索引模型是向量空间模型的延伸


### LSI模型
LSI模型应用于信息检索相对于基本的向量空间模型有哪些优势？

解决了具有一词多义和一义多词对检索造成的检索效果不好

各矩阵含义：

词项C——文档矩阵可以分解成3个矩阵的乘积

词项矩阵U——每个词项对应其中的一个行向量

文档矩阵$V^{T}$ ——每篇文档对应其中的一个列向量

奇异值矩阵Σ——对角方阵，对角线上的奇异值代表的是 每个“语义”维度的重要性

## 9）基于深度学习的自然语言处理

### 词汇表征

#### one-hot
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613141811.png)

可见 one-hot 词汇表征方法最后形成的结果是一种稀疏编码结果，在深度学习应用于 NLP 任务之前，这种表征方法在传统的 NLP 模型中已经取得了很好的效果。

缺陷：
* 一是容易造成维数灾难
* 不能很好的词汇与词汇之间的相似性
#### 词嵌入技术
将词汇嵌入到了一种数学空间里面，所以叫做词嵌入。语义相似的词趋向于出现在相似的上下文。因此在学习过程中，这些向量会努力捕捉词的邻近特征，从而学习到词汇之间的相似性。有了词向量，便能够通过计算余弦距离等方式来度量词与词之间的相似度。

### 经典的NLP模型
#### Word2vec模型
跳词模型
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613143517.png)
连续词袋模型
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613143531.png)

为什么要进行近似训练？

无论是跳词模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包涵词典大小数目的项的累加。对含有几十万上百万的较大词典，每次的梯度计算开销可能过大。为了降低计算复杂度，引入近似训练方法。

近似训练方法：
* 负采样：通过采样并添加负类样本使目标函数更有意义。使训练中每一步的梯度计算开销不再与词典大小相关，而与K（噪声词数量）线性相关。当K取较小的常数时，负采样在每一步的梯度计算开销较小。
* 层序softmax：使用二叉树这一数据结构，计算复杂度降为$O(log_{2}|V|)$,当词典V很大时，计算开销大幅度降低

#### Transformer-based Model
Google提出了Transformer，完全抛弃了CNN和RNN，只基于注意力机制捕捉输入和输出的全局关系，框架更容易并行计算。
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613145600.png)

#### ElMo模型
优势：
* ELMo能够学习到词汇用法的复杂性，比如语法、语义。
* ELMo能够学习不同上下文情况下的词汇多义性。

两个步骤
* 第一个阶段是利用语言模型进行预训练
* 第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中

#### Bert模型
模型输入：
* Token embedding 字向量：将文本每个字转换为一维向量
* Segment Embedding 文本向量：用于刻画文本的全局语义信息
* Position Embedding 位置向量：对不同位置的字/词附加一个不同的向量以作区分
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613150316.png)

两个任务：
* Masked Language Model：为了训练双向特征，随机去掉句子中的部分token，然后模型来预测被去掉的token是什么
* Next Sentence Prediction：为了让模型捕捉两个句子的联系，使模型具备理解长序列上下文的能力，使用此方法预测第二句话是否是第一句话的下一句话

微调：

在下游任务中对Bert进行微调是比较简单的。一般只需要增加一层即可。

### 领域知识图谱与NLP在其应用
#### 领域知识图谱和通用知识图谱的区别
* 从广度来看，通用知识图谱涵盖的范围明显大于领域知识图谱。
* 从深度来看，领域知识图谱通常更深
* 领域知识图谱通常涵盖细粒度的知识
#### 领域知识图谱构建流程
![](https://pic-imge.oss-cn-hangzhou.aliyuncs.com/img/20220613151750.png)

