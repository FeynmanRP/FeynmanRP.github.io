> First version on 2021-12-5
>
>Last version on 2021-12-5
>
>Author :FeynmanRP

- [绪论](#绪论)
  - [基本术语](#基本术语)
  - [假设空间](#假设空间)
  - [归纳偏好](#归纳偏好)
    - [NFL定理（No Free Lunch Theorem）](#nfl定理no-free-lunch-theorem)

# 绪论

## 基本术语
就像人学习需要做做题目刷卷子一样，机器也是要有学习资料，我们通常把它称作数据集合，以下是相关介绍

* 数据集：我们用于训练的数据集合
  * 示例/样本：数据集中的一个对象
  * 属性/特征：反应事件或对象的表现或特征
  * 属性值：属性的具体取值
  * 属性空间/样本空间：属性张成的空间
  * 特征向量：每一个样本的属性在属性空间中对应一个坐标向量，我们称其为特征向量
  * 维数：样本的属性个数也称为维数
  
  
此外，光有数据还不够，我们把数据分成两块，一块用来训练机器的模型，一块用来检验模型的好坏，分别称之为训练集与预测集。


* 学习/训练：从数据中学得模型的过程
  * 训练数据：训练过程中使用的数据
  * 训练样本：训练数据中每一个样本
  * 训练集：训练样本组成的集合
  * 假设：数据的某种潜在规律
  * 标记：训练样本的结果信息（一般为人为打上标记）
  * 样例：拥有了标记的示例
  * 标记空间/输出空间：样例的集合
  

* 测试：学习模型后，使其进行预测的过程
  * 测试样本：被预测的样本
  * 泛化性能：模型能很好的适用于整个样本空间（一般来说，训练样本越多，越有可能获得较强泛化能力的模型）

若我们预测的是离散值，则此类学习任务成为分类，若预测的是连续值，则称为回归。根据训练数据是否拥有标记信息，可把学习任务分为监督学习与无监督学习。

## 假设空间
我们可以把学习看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配的假设”，即能够将训练集中的瓜判断正确的假设。可能存在着多个与训练集一致的假设集合，我们称之为版本空间。

## 归纳偏好
### NFL定理（No Free Lunch Theorem）
无论多么高明的算法，也只能在特定领域有效，并不能适用于所有领域。脱离具体问题，空泛的谈论“什么学习算法更好”毫无意义


